\documentclass[12pt,letterpaper]{article}
\usepackage[margin=.65in]{geometry}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{mathtools}
\usepackage{siunitx}
\usepackage{multicol}
\usepackage{grffile}
\usepackage{hyperref}
\usepackage{pbox}
\usepackage{qtree}
\usepackage{tikz}
\usepackage{tkz-graph}
\usepackage{listings}
%\usepackage{slashbox}
\usepackage{array}
%\usepackage{logicsym}
\usepackage{amssymb}
\usepackage{etoolbox}
\usepackage{pdfpages}
\usepackage{longtable}

\lstset{basicstyle=\ttfamily,
escapeinside={||},
mathescape=true}

\sisetup{per-mode=symbol, group-digits=false, separate-uncertainty=true}

\renewcommand{\tt}[1]{\texttt{#1}}

%Centered multi-row box
\newcommand{\cmrb}[3][0.8in]{\parbox{#1}{\centering #2 \\ #3 } }

%Circled character
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\makeatletter
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{5}}
\makeatother

\everymath{\displaystyle}

\newcolumntype{T}{p{2in}<{\ttfamily}}

\newcommand{\sh}{\tt{store\_handler}}
\renewcommand{\sp}{\tt{storage\_process}}
\newcommand{\gs}{\tt{gen\_server}}

\author{Alejandro Frias \and Ravi Kumar \and David Scott}
\date{\today}
\title{Distributed Key-Value Storage Algorithm\\CS181E --- Distributed Systems\\Assignment 5}
\begin{document}
\maketitle


\onehalfspacing
\section*{Algorithm Description}


\setlength{\extrarowheight}{5pt}
\setlength{\LTcapwidth}{\textwidth}

\begin{longtable}{p{4.5in}T}
\caption{Caption}
\label{msgTable}\\
\textbf{Message Description} & \textbf{Erlang Pattern} \\
\hline
\endhead
\hline \multicolumn{2}{|r|}{{This table continues on the next page...}} \\ \hline
\endfoot
\endlastfoot

Store Request. Sent by either OW or a \sp to a \sp to store \tt{value} for \tt{key}. These are only ever received by storage processes in the network. When an \sp receives such a message, it first checks if the hashed key is equal to its own ID. If it is, then it stores the value for the specified key, and sends a \tt{backup\_store} message to its \sh. If not, it forwards it to the closest storage process to the destination (closest here meaning nearest process \textit{before} the destination, since processes can only send messages forward in the ring). &
\{Pid, Ref, store, Key, Value\} \\

Stored Confirmation. Sent by an \sh to OW after the corresponding request has been stored in the proper storage process and the data has been backed up in the \sh sending the message. &
\{Ref, stored, Old\_Value\} \\

Retrieve Request. Sent by OW and storage processes; received by storage processes. When a storage processes receives such a message, it checks if the hash of the key is equal to its own process ID. If it is, the storage process has the value for that key, and replies with a retrieve response. If not, it forwards it to the closest storage process to the destination (closest as defined above). &
\{Pid, Ref, retrieve, Key\} \\

Retrieve Response. Sent by storage processes to the OW. After a storage process receives a retrieve request meant for it, it looks up the relevant value and reports it to the requesting process in the OW. &
\{Ref, retrieved, Value\} \\

First Key Request. Sent by the OW to storage processes, and by storage processes to storage handlers. When a storage process receieves this, it will forward the message up to its storage handler. When such a message is received by a storage handler, it will start a first key computation by adding the ref to its list of ongoing computations and sending a First Key Computation message to the next node in the ring. &
\{Pid, Ref, first\_key\} \\

Last Key Request. Sent by the OW to storage processes, and by storage processes to storage handlers. When a storage process receieves this, it will forward the message up to its storage handler. When such a message is received by a storage handler, it will start a last key computation by adding the ref to its list of ongoing computations and sending a Last Key Computation message to the next node in the ring. &
\{Pid, Ref, last\_key\} \\

Num Keys Request. Sent by the OW to storage processes, and by storage processes to storage handlers. When a storage process receieves this, it will forward the message up to its storage handler. When such a message is received by a storage handler, it will start a num keys computation by adding the ref to its list of ongoing computations and sending a Num Keys Computation message to the next node in the ring. &
\{Pid, Ref, num\_keys\} \\

Node List Request. Sent by the OW to storage processes, and by storage processes to storage handlers. When a \sp receieves this, it will forward the message up to its \sh When such a message is received by a \sh, it will query the global registry for the list of nodes and report that data to the requester. &
\{Pid, Ref, node\_list\} \\

Request Result. Sent to the OW by a storage handler. This reports the result of a First Key, Last Key, Num Keys, or Node List Request to the original requester after the storage handlers have finished computing the result. &
\{Ref, result, Result\} \\

Failure Notification. Sent to the OW by storage handlers or storage processes to notify the OW that a particular computation has failed. &
\{Ref, failure\} \\

Leave Request. Sent by the OW to storage processes and by storage processes to storage handlers. When received by a \sp, it forwards the message to its \sh. When received by an \sh, it immediately kills all storage processes on the node it is running on, and kills itself. &
\{Pid, Ref, leave\} \\

Backup Store Request. Sent by \sh and \sp, and received by \sh. If a \sh receives this message from a \sp, it forwards the message to the next \sh. If an \sh receives this message from another \sh, it will back up the data in the message, then notify the OW of the store's success and the old value. & 
\{Pid, Ref, backup\_store, Key, Value, ProcessID\} \\

Messages About Keys.  Sent and received by \sh.  If \tt{Ref} is in the list of the receiver's in-progress computations, the computation is over and the received message contains the result. The receiver will then send the result \tt{ComputationSoFar} back to the OW.  Otherwise, it will update \tt{ComputationSoFar} with its relevant value and forward the message to the next node's \sh.  &
\{Pid, Ref, *\_key, ComputationSoFar\} \\

Joining Behind. Received and sent by \sh. A \sh will send this when it is joining to the next node's \sh to indicate that it is joining behind the recipient in the ring. When received, send all stored backup data to the sender, then delete all backup data for processes numbered less than \tt{NodeID}. &
\{Pid, joining\_behind, NodeID\} \\

Joining in Front.  Received and sent by \sh. A \sh will send this when it is joining to the previous node's \sh to indicate that it is joining in front of the recipient in the ring. When receiving such a message, % if your ID is equal to \tt{DestID}, 
kill the data storage processes that the new node is now running (i.e. the ones numbered from \tt{NodeID} to the ID of the node after the new one. &
\{joining\_front, NodeID, DestID\} \\

Node with \tt{NodeID} Died.  Received by \sh, sent by a \gs{} listener started by that particular \sh. This is a notification that the node behind the receiving node has stopped running. When such a message is received, the \sh{} changes the node's ID to \tt{NodeID}, then uses all of the backup data it's holding to start up new data storage processes.  Then it deletes the backup data, and sends a \tt{backup\_request} message around the ring, to get the data it should be backing up from the node behind it. & \{died, NodeID\} \\

Backup Node Data.  Received and sent by \sh.  When received, add all the data to existing backup data. Send by a node $A$'s predecessor when node $A$ died and $A$'s successor is taking over for it.&
%Sent to a node $A$'s successor when $A$'s predecessor died and $A$ is taking over for its predecessor.  & 
\{backup\_node, Data\} \\

Backup Request.  Received and sent by \sh.  If it is received on the node with \tt{DestID}, send each of this node's \sp es an \tt{all\_data} message.  After compiling all of the results from those requests, send all of this node's stored data to this node's successor node in a \tt{backup\_node} message.  If this node is not \tt{DestID}, just forward the request message to the next node's \sh. Initially sent by a node which stepped into the void left by a node that died.  & 
\{backup\_request, DestID\} \\

All data request message.  Received by \sp{} and sent by \sh.  When received by a \sp, respond with an \tt{all\_data\_send} message containing all this \sp's data.  & 
\{all\_data, Pid\} \\

All data send message.  Received by \sh{} and sent by \sp.  The \sh{} adds the received data to an ongoing list of data and removes the sender from the list of processes it is waiting for. If it's the last response that was being waited for, send the \tt{backup\_node} message to the next node. & 
\{all\_data, Data, Pid\}

\end{longtable}

\clearpage
\onehalfspacing

\section*{Our Setup and Some Correctness}
The setup:
Each node has a backup of the node preceding it, held by a non-storage process for that node that we call the handler, since it also handles much of the inter-system communication. Each node is listening to the preceding node for crashes. In this way each node has one node that it is responsible for and one that is responsible for it. \\
When a node enters the system, it does so in the middle of the node that is responsible for the most storage processes (breaking ties arbitrarily by process ID number). The new node grabs the entire backup data of all the storage processes of the node that this new node is entering in the middle of from the next node, and in the process tells that next node to delete the first half of its data since it will be backing it up. It uses the first half of that data to create its backup and the other half to start the storage processes it will be responsible for. In this way we minimize the number of nodes a joining node talks to to two (sorry for that sentence). \\
When a node crashes or leaves, the next node finds out having been the only one listening to it. Since it has the entire backup data of that node, it can immediately startup all of the processes that just died. So it takes over the dead node, changing it's own node number to the one of the deceased. Then, since we lost some backup data of the dead node, it sends a request around the ring to the node just previous to it to get all the data on that node's processes. This takes a maximum of $m$ messages, where there are $2^m$ processes. In this way we can get the system back up and running immediately and then, while still accepting requests from the outside world, start rebuilding the back up. \\
To ensure redundancy, a storage process never communicates back to the outside world directly when a store request comes in. Instead, after forwarding it to the correct storage process, it stores the new key-value pair and tells it's nodes handler process that it did so. The handler then sends this to the next node to be backed up. That next node will then notify the outside world that the value has been stored after it stores the backup. This ensures every store request has been backed up before letting the outside world know. Also, very few messages need to be passed. It takes a maximum of $m$ messages to get the store request to the right storage process and then a couple more after that to back it up and respond to the outside world, since the next node is guaranteed to be visible to the node that has that storage process (part of why we chose to have nodes store the back up of the previous node). \\
Messages about the system, like first-key and last-key are forwarded directly to the handler process to take care of. The process will start a message around that will go around the ring and will store that node's contribution to calculating the first key or number of keys. Since each handler has a backup of the previous node, it doesn't need to communicate directly with each of the processes, but instead just sees if the first-key that has been found is better or worse than it's first key, or similar comparison for the other key requests. Once a handler gets it's message back, it can tell the outside person the result of the snapshot. These system processes scale with the number of nodes and are guaranteed to work if the a node doesn't crash mid computation. Each node's computation will be constant since it can keep track of the first key, the last key, and the number of keys in it's back up as store requests come in. And since a completely stored value is only considered complete once the handler has backed up one up, we're guaranteed to be accurate. \\
Retrieve requests are the only messages that storage processes deal with completely without the handler. They simply forward the message along the closest chord or if they have the value (or should have the value), return the result to the outside world.

\end{document}
